## Installation

### Clone Repository

```sh
git clone https://github.com/FlagOpen/FlagScale.git
cd FlagScale/
```

### Setup Conda Environment

Create a new conda environment for robotics training:

```sh
conda create -n flagscale-train python=3.12
conda activate flagscale-train
```

Install FlagScale and robotics dependencies:

```sh
cd FlagScale/
pip install . --verbose
pip install -r requirements/train/robotics/requirements.txt
pip install git+https://github.com/NVIDIA/Megatron-Energon.git@ab40226
```

Install Megatron:

```sh
mkdir -p /tmp
cd /tmp
git clone https://github.com/flagos-ai/Megatron-LM-FL.git
cd Megatron-LM-FL
pip install --no-build-isolation .[mlm,dev]

# add your path of FlagScale and the Megatron in FlagScale to PYTHONPATH
export PYTHONPATH=$PYTHONPATH:/xxx/FlagScale:/xxx/FlagScale/flagscale/train/
```

## Training

### Download Model

```sh
git lfs install

mkdir -p /models/BAAI/
cd /models/BAAI/
git clone https://huggingface.co/BAAI/RoboBrain-X0-Preview
```

If you don't have access to the international internet, download from modelscope.

```sh
mkdir -p /models/
cd /models/
modelscope download --model BAAI/RoboBrain-X0-Preview --local_dir BAAI/RoboBrain-X0-Preview
```

### Prepare Dataset

FlagScale uses WebDataset format and Megatraon.Energon data loader, you need process your data first.

There is a dataset processed: [demo_0913_n2](https://gitee.com/hchnr/flag-scale/tree/robotics_dataset/demo_0913_n2/wds-256).

Download demo_0913_n2:

```sh
mkdir /tmp/datasets
cd /tmp/datasets
git clone https://gitee.com/hchnr/flag-scale.git
cd flag-scale
git checkout robotics_dataset
```

Move .jpg and .npy files from ./demo_0913_n2/deps to /:

```sh
mkdir -p /share/
cp -r ./demo_0913_n2/deps/* /
```

The directory structure of demo_0913_n2 is as follows:
- build_dep.sh: Copy .npy and .jpg files from production environment to ./deps
- demo_0913_n2.jsonl: Timesteps, including: task(str), images(.jpg), action(.npy), state(.npy)
- deps: .npy and .jpg files
- wds-2: Data in webdataset format (DP=2), generated by tools/datasets/vla/convert.py

If you need to make your own datasets, generate Data in webdataset format (DP=2) to ./demo_0913_n2/wds-2:

```sh
python tools/datasets/vla/convert.py \
    --dataset-root=./demo_0913_n2 \
    --output-root=./demo_0913_n2 \
    --json=demo_0913_n2.jsonl \
    --train-split 1 \
    --val-split 0 \
    --images-key=image \
    --videos-key=video \
    --vision-root='' \
    --shuffle-tars \
    --num-workers=1 \
    --max-samples-per-tar 100000 \
    --dp-size 2
```

### Edit Config

```sh
cd FlagScale/
vim examples/robobrain_x0/conf/train/robobrain_x0.yaml
```

Change 2 fields:
- data.data_path -> /tmp/datasets/flag-scale/demo_0913_n2/wds-1
- data.tokenizer -> /models/BAAI/RoboBrain-X0-Preview

### Start Training
```sh
cd FlagScale/
python run.py --config-path ./examples/robobrain_x0/conf/ --config-name train action=run
```

## Serving

### Download Tokenzier

```sh
mkdir -p /models/physical-intelligence/
cd /models/physical-intelligence/
git lfs install
git clone https://huggingface.co/physical-intelligence/fast
```

### Edit Config

```sh
cd FlagScale/
vim examples/robobrain_x0/conf/serve/robobrain_x0.yaml
```

Change 3 fields:
- engine_args.model_sub_task -> /models/BAAI/RoboBrain-X0-Preview
- engine_args.port -> A port available in your env, for example: 5001
- engine_args.tokenizer_path ->/models/physical-intelligence/fast

### Run Serving

```sh
cd FlagScale/
python run.py --config-path ./examples/robobrain_x0/conf --config-name serve action=run
```

### Test Server with Client

Download test images:

```sh
cd FlagScale/
wget https://gitee.com/hchnr/flag-scale/raw/robotics_dataset/orbbec_0_latest.jpg
wget https://gitee.com/hchnr/flag-scale/raw/robotics_dataset/orbbec_1_latest.jpg
wget https://gitee.com/hchnr/flag-scale/raw/robotics_dataset/orbbec_2_latest.jpg
```

Run client:

```sh
python examples/robobrain_x0/client_agilex.py  \
--host 127.0.0.1 \
--port 5001 \
--base-img orbbec_0_latest.jpg \
--left-wrist-img orbbec_1_latest.jpg \
--right-wrist-img orbbec_2_latest.jpg \
--num-steps 20
```
